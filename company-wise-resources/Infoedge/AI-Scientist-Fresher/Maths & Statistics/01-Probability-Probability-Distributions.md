# Maths & Statistics Comprehensive Revision - AI Scientist Interview Prep

## Interview Overview
- **Weightage**: 33.33% (Critical Section!)
- **Time**: 30 minutes for Round 1
- **Focus**: Deep conceptual understanding with practical applications
- **What Interviewers Want**: Clear reasoning, step-by-step approach, intuition behind formulas

## Table of Contents
1. [Probability](#probability)
2. [Probability Distributions](#probability-distributions)
3. [Sampling](#sampling)
4. [Correlations](#correlations)
5. [Hypothesis Testing](#hypothesis-testing)
6. [Statistical Significance](#statistical-significance)
7. [Central Limit Theorem](#central-limit-theorem)
8. [Paired Means Tests](#paired-means-tests)
9. [Bayes' Theorem](#bayes-theorem)
10. [Linear Algebra](#linear-algebra)

---

## Probability

### Fundamental Concepts (Must Know for Interview)

#### 1. Basic Probability Definition
**Intuition**: Probability measures how likely something is to happen, ranging from 0 (impossible) to 1 (certain)

**Mathematical Definition**:
P(A) = Number of favorable outcomes √∑ Total possible outcomes

**Why This Matters**: This is the foundation of all statistical thinking in ML and data science.

#### 2. Sample Space and Events
```
Sample Space (S): All possible outcomes
Event (A): Subset of sample space
Complement (A'): All outcomes NOT in A
```

**Example**: Rolling a die
- Sample Space S = {1, 2, 3, 4, 5, 6}
- Event A = "even number" = {2, 4, 6}
- Complement A' = "odd number" = {1, 3, 5}

#### 3. Conditional Probability - The Heart of Many ML Algorithms

**Intuition**: "What's the probability of A happening, given that B has already happened?"

**Formula**: P(A|B) = P(A ‚à© B) / P(B)

**Step-by-Step Thought Process**:
1. What's the new "world" we're living in? (B has happened)
2. Within this new world, what fraction corresponds to A?
3. This becomes our conditional probability

**Why Crucial**: Naive Bayes, Bayesian networks, medical diagnosis, spam filters all depend on this!

#### 4. Independence - A Special Case

**Intuition**: Two events don't influence each other at all

**Mathematical Condition**: P(A|B) = P(A) OR P(B|A) = P(B)
**Practical Formula**: P(A ‚à© B) = P(A) √ó P(B)

**Common Mistake**: Assuming independence when it's not true!

### Key Probability Rules with Intuition

#### 1. Addition Rule (OR scenarios)
**When to Use**: "What's probability of A OR B happening?"

```
General Case: P(A ‚à™ B) = P(A) + P(B) - P(A ‚à© B)
Special Case (Mutually Exclusive): P(A ‚à™ B) = P(A) + P(B)
```

**Intuition**: Add probabilities, but subtract overlap to avoid double-counting!

**Interview Tip**: Always check if events are mutually exclusive first!

#### 2. Multiplication Rule (AND scenarios)
**When to Use**: "What's probability of A AND B both happening?"

```
General Case: P(A ‚à© B) = P(A) √ó P(B|A)
Special Case (Independent): P(A ‚à© B) = P(A) √ó P(B)
```

**Intuition**: First probability of A, then probability of B given A has happened

### Deep Dive Interview Questions

#### Q1: Two Dice Problem (Foundation Level)
**Question**: Two fair dice are rolled. What's the probability that the sum is exactly 7?

**Step-by-Step Solution**:
1. **Sample Space**: 6 √ó 6 = 36 total outcomes
2. **Favorable Outcomes**: (1,6), (2,5), (3,4), (4,3), (5,2), (6,1) = 6 outcomes
3. **Probability**: 6/36 = 1/6

**Interview Follow-ups**:
- What about sum > 7? (15 favorable outcomes ‚Üí 15/36 = 5/12)
- What about sum is prime? (2,3,5,7,11 ‚Üí count these)
- What if dice are biased? How would approach change?

#### Q2: Card Drawing Without Replacement (Tests Conditional Probability)
**Question**: From a standard deck, you draw 2 cards without replacement. What's P(both are hearts)?

**Intuitive Approach**:
1. **First draw**: P(heart) = 13/52 = 1/4
2. **Second draw**: Given first was heart, now 12 hearts left, 51 cards total
3. **Second draw**: P(heart|first was heart) = 12/51
4. **Both events**: P = (13/52) √ó (12/51) = 1/17

**Alternative Solution Using Combinations**:
- Total ways to choose 2 cards: C(52,2) = 1326
- Ways to choose 2 hearts: C(13,2) = 78
- Probability: 78/1326 = 1/17

**Interview Tip**: Show both methods - demonstrates versatility!

#### Q3: Birthday Problem (Classic Interview Question)
**Question**: In a room of 23 people, what's the probability that at least 2 people share the same birthday?

**Intuitive Strategy**: Calculate the opposite probability (NO shared birthdays), then subtract from 1.

**Step-by-Step Solution**:
1. **Person 1**: Can have any birthday (365/365)
2. **Person 2**: Must have different birthday (364/365)
3. **Person 3**: Must avoid previous 2 birthdays (363/365)
4. **Continue**... Person 23: (343/365)
5. **Multiply all together**: (365√ó364√ó363√ó...√ó343)/365¬≤¬≥
6. **Result**: ‚âà 0.4927 (49.27% chance NO shared birthdays)
7. **Final Answer**: 1 - 0.4927 = 0.5073 (50.73% chance at least 2 share birthday)

**Why This Interview Question**:
- Tests understanding of complementary probability
- Shows how small individual probabilities multiply to significant results
- Demonstrates counterintuitive nature of probability

**Follow-up Questions**:
- How many people needed for 90% probability? (‚âà 41 people)
- What about leap years? How does this affect the calculation?
- How would you estimate this without exact calculation?

#### Q4: Medical Testing (Critical for Data Science Applications)
**Question**: A disease affects 1% of population. A test is 99% accurate. If you test positive, what's the probability you actually have the disease?

**This is a Bayes' Theorem problem in disguise! Let's solve intuitively:**

**Step 1: Assume 10,000 people (easy numbers)**
- **Actually sick**: 100 people (1% of 10,000)
- **Actually healthy**: 9,900 people

**Step 2: Test results**
- **True positives**: 99 sick people √ó 99% accuracy = 99 people
- **False positives**: 9,900 healthy √ó 1% error rate = 99 people
- **Total positive tests**: 99 + 99 = 198 people

**Step 3: Final probability**
- **Actually sick given positive test**: 99/198 = 50%

**Why This is Shocking**: Only 50% chance despite 99% accurate test!

**Interview Insight**: This demonstrates why base rates are crucial in ML and data science.

#### Q5: Monte Hall Problem (Tests Conditional Thinking)
**Question**: You're on a game show. 3 doors, 1 car behind 1 door. You pick door 1. Host opens door 3 showing goat. Should you switch to door 2?

**Intuitive Solution**:
```
Initial choice: Door 1 (1/3 probability)
Switching strategy wins if car was behind doors 2 or 3
- Car behind door 2: Host opens door 3, you switch ‚Üí WIN
- Car behind door 3: Host opens door 2, you switch ‚Üí WIN
- Car behind door 1: Host opens either door, you switch ‚Üí LOSE

Switching wins 2/3 of the time!
```

**Key Insight**: Host's action provides information that changes probabilities!

### Advanced Probability Concepts

#### 1. Expected Value (Foundation of ML)
**Intuition**: Long-term average if you repeat experiment many times

**Formula**: E[X] = Œ£(x √ó P(x))

**Example**: Roll a fair die, win amount shown
E = 1√ó(1/6) + 2√ó(1/6) + 3√ó(1/6) + 4√ó(1/6) + 5√ó(1/6) + 6√ó(1/6) = 3.5

**Why Important**: Decision theory, reinforcement learning, expected utility

#### 2. Variance and Standard Deviation
**Intuition**: How spread out are the values?

**Formula**: Var(X) = E[(X - Œº)¬≤] = E[X¬≤] - (E[X])¬≤
**Standard Deviation**: œÉ = ‚àöVar(X)

#### 3. Covariance and Correlation Preview
```
Cov(X,Y) = E[(X - Œº‚Çì)(Y - Œº·µß)]
Correlation = Cov(X,Y) / (œÉ‚Çì √ó œÉ·µß)
```

### Interview Strategy for Probability Questions

#### What Interviewers Look For:
1. **Clear problem restatement**: "Let me understand what we're trying to find..."
2. **Step-by-step approach**: Break down complex problems
3. **Multiple solution methods**: Show flexibility in thinking
4. **Intuition behind formulas**: Not just memorization
5. **Connection to real applications**: How does this apply to ML/data science?

#### Common Traps to Avoid:
‚ùå Assuming independence without checking
‚ùå Confusing P(A|B) with P(B|A)
‚ùå Forgetting to consider all possible cases
‚ùå Making arithmetic errors under pressure
‚ùå Not explaining your thought process

#### Pro Tips:
‚úÖ Always verify your answer makes sense (0 ‚â§ probability ‚â§ 1)
‚úÖ Use complementary probability when it's easier
‚úÖ Draw diagrams or tables when helpful
‚úÖ Practice mental math for common fractions
‚úÖ Remember key probabilities: 1/2, 1/3, 1/4, 1/6, 1/8

### Real Interview Scenarios

#### Scenario 1: A/B Testing
**Question**: "We ran an A/B test. Version A got 100 conversions from 1000 users, Version B got 120 from 1000. Is this statistically significant?"

**Approach**: This leads to hypothesis testing, but starts with probability concepts!

#### Scenario 2: ML Model Evaluation
**Question**: "Our spam filter has 95% accuracy. Only 2% of emails are spam. What's P(email is spam | flagged as spam)?"

**Connection**: Direct application of Bayes' Theorem!

#### Scenario 3: Quality Control
**Question**: "A factory produces items with 1% defect rate. We test 5 items. What's P(at least one defect)?"

**Strategy**: Use complement: 1 - P(no defects in all 5)

### Quick Reference Cheat Sheet

| Formula | When to Use | Key Insight |
|---------|-------------|-------------|
| P(A|B) = P(A‚à©B)/P(B) | Conditional probability | Updates knowledge |
| P(A‚à©B) = P(A)√óP(B) | Independent events | Events don't affect each other |
| P(A‚à™B) = P(A)+P(B)-P(A‚à©B) | OR scenarios | Avoid double counting |
| E[X] = Œ£(x√óP(x)) | Expected value | Long-term average |
| 1 - P(no success) | At least one success | Often easier calculation |

---

## Probability Distributions

### Why Probability Distributions Matter in AI/ML

**Core Insight**: Most ML algorithms assume or learn probability distributions. Understanding them helps you:
- Choose the right model for your data
- Understand model limitations
- Debug when things go wrong
- Optimize hyperparameters

### Two Main Categories

#### 1. Discrete Distributions (Countable Outcomes)
**When to Use**: When you can count possible outcomes
**Examples**: Number of customers, defects, emails, etc.

#### 2. Continuous Distributions (Infinite Outcomes)
**When to Use**: When outcomes are measurements
**Examples**: Height, weight, time, temperature, etc.

### Deep Dive: Discrete Distributions

#### 1. Bernoulli Distribution - The Building Block
**Intuition**: Single trial with success/failure outcome

**Mathematical Definition**:
```
P(X = 1) = p (success)
P(X = 0) = 1 - p (failure)
```

**Key Properties**:
- **Mean**: E[X] = p
- **Variance**: Var(X) = p(1 - p)
- **PMF**: P(X = k) = p·µè(1-p)¬π‚Åª·µè for k ‚àà {0,1}

**Real-World Applications**:
- Email spam/not spam classification
- Medical test positive/negative
- Customer purchase/no purchase

**Interview Question**: "A coin has P(heads) = 0.6. What's the expected number of heads in 3 flips?"
**Answer**: Each flip is Bernoulli(0.6), so E[total] = 3 √ó 0.6 = 1.8

#### 2. Binomial Distribution - Multiple Bernoulli Trials
**Intuition**: Counting successes in fixed number of independent trials

**Mathematical Definition**:
```
X ~ Bin(n, p) where:
n = number of trials
p = probability of success
X = number of successes
```

**Probability Mass Function**:
P(X = k) = C(n, k) √ó p·µè √ó (1-p)‚Åø‚Åª·µè

Where C(n,k) = n! / (k! √ó (n-k)!) is the combination formula

**Step-by-Step Intuition**:
1. Choose which k trials are successes: C(n,k) ways
2. Each of those k successes happens with probability p·µè
3. Each of the (n-k) failures happens with probability (1-p)‚Åø‚Åª·µè
4. Multiply all together!

**Key Properties**:
- **Mean**: E[X] = n √ó p
- **Variance**: Var(X) = n √ó p √ó (1-p)
- **Shape**: Symmetric when p=0.5, skewed otherwise

**Real-World Applications**:
- Quality control: defective items in batch
- Clinical trials: patients who respond to treatment
- Marketing: customers who click on ad

#### Binomial Interview Questions

##### Q1: Email Campaign Analysis
**Question**: An email campaign has 5% conversion rate. You send to 200 people. What's P(exactly 10 conversions)?

**Step-by-Step Solution**:
1. **Identify**: This is Binomial(n=200, p=0.05)
2. **Formula**: P(X=10) = C(200,10) √ó 0.05¬π‚Å∞ √ó 0.95¬π‚Åπ‚Å∞
3. **Calculation**: ‚âà 0.087 or 8.7%

**Interview Insight**: This tests understanding of when binomial applies!

**Follow-up Questions**:
- What's P(at least 5 conversions)? (Use 1 - P(X ‚â§ 4))
- What's expected number of conversions? (200 √ó 0.05 = 10)
- What if conversion rate is unknown? How would you estimate it?

##### Q2: Quality Control Problem
**Question**: A factory produces items with 2% defect rate. What's probability that in 50 items, at most 1 is defective?

**Solution**:
P(at most 1) = P(0) + P(1)
P(0) = C(50,0) √ó 0.02‚Å∞ √ó 0.98‚Åµ‚Å∞ ‚âà 0.364
P(1) = C(50,1) √ó 0.02¬π √ó 0.98‚Å¥‚Åπ ‚âà 0.372
Total ‚âà 0.736 or 73.6%

#### 3. Poisson Distribution - Rare Events
**Intuition**: Number of events in fixed time/space interval when events are rare and independent

**Mathematical Definition**:
```
X ~ Poisson(Œª) where:
Œª = average rate of events
X = number of events in interval
```

**Probability Mass Function**:
P(X = k) = (Œª·µè √ó e‚ÅªŒª) / k!

**When to Use Poisson (CRITICAL FOR INTERVIEW)**:
‚úÖ Events are independent
‚úÖ Average rate is constant
‚úÖ Two events can't happen at exactly same time
‚úÖ Events are rare relative to observation period

**Key Properties**:
- **Mean**: E[X] = Œª
- **Variance**: Var(X) = Œª (unique property!)
- **Shape**: Right-skewed, becomes more symmetric as Œª increases

**Real-World Applications**:
- Customer arrivals per hour
- Website hits per minute
- Defects per square foot
- Calls to call center

#### Poisson Interview Questions

##### Q1: Call Center Analysis
**Question**: Call center receives 2 calls per minute on average. What's P(exactly 3 calls in next minute)?

**Solution**:
Œª = 2, k = 3
P(X=3) = (2¬≥ √ó e‚Åª¬≤) / 3! = (8 √ó e‚Åª¬≤) / 6 ‚âà 0.180

**Follow-up**:
- P(no calls in 3 minutes)? (Œª=6, P(X=0) = e‚Åª‚Å∂ ‚âà 0.00248)
- P(at least 1 call in 30 seconds)? (Œª=1, P(X‚â•1) = 1 - e‚Åª¬π ‚âà 0.632)

##### Q2: Website Traffic
**Question**: Website gets 50 hits per hour. What's P(60 or more hits in an hour)?

**Interview Strategy**:
1. **Identify**: Poisson(Œª=50)
2. **Challenge**: Direct calculation of P(X‚â•60) requires summing many terms
3. **Practical Approach**: Use normal approximation or computational tool
4. **Key Insight**: Show understanding of approximation methods

### Deep Dive: Continuous Distributions

#### 1. Normal Distribution - The Most Important One
**Intuition**: Bell-shaped curve that appears everywhere in nature

**Mathematical Definition**:
```
X ~ N(Œº, œÉ¬≤) where:
Œº = mean (center)
œÉ¬≤ = variance (spread)
œÉ = standard deviation
```

**Probability Density Function**:
f(x) = (1/‚àö(2œÄœÉ¬≤)) √ó e^(-(x-Œº)¬≤/(2œÉ¬≤))

**Why Normal Distribution is CRUCIAL**:
1. **Central Limit Theorem**: Means of samples become normal
2. **Many natural phenomena follow it**
3. **Foundation of statistical inference**
4. **Many ML algorithms assume normality**

**Key Properties**:
- **Symmetric**: Mean = Median = Mode = Œº
- **68-95-99.7 Rule**:
  - 68% within Œº ¬± œÉ
  - 95% within Œº ¬± 2œÉ
  - 99.7% within Œº ¬± 3œÉ

**Standard Normal Transformation**:
```
Z = (X - Œº) / œÉ ~ N(0, 1)
```

**Interview Tip**: Always standardize when working with normal distributions!

#### Normal Distribution Interview Questions

##### Q1: Height Distribution
**Question**: Male heights follow N(175, 25) cm¬≤. What % are taller than 185 cm?

**Step-by-Step Solution**:
1. **Standardize**: Z = (185-175)/‚àö25 = 10/5 = 2
2. **Lookup**: P(Z > 2) = 1 - P(Z ‚â§ 2) = 1 - 0.9772 = 0.0228
3. **Answer**: 2.28% are taller than 185 cm

**Follow-up Questions**:
- What height marks the 90th percentile? (Find z for 0.90, then x = Œº + zœÉ)
- What's P(height between 170 and 180 cm)?
- If we sample 100 men, what's distribution of average height?

##### Q2: Quality Control Tolerance
**Question**: Part lengths follow N(10, 0.04) cm¬≤. Acceptable range: 9.8 to 10.2 cm. What % of parts are acceptable?

**Solution**:
1. **Lower bound**: Z‚ÇÅ = (9.8-10)/0.2 = -1
2. **Upper bound**: Z‚ÇÇ = (10.2-10)/0.2 = 1
3. **Probability**: P(-1 < Z < 1) = P(Z < 1) - P(Z < -1) = 0.8413 - 0.1587 = 0.6826
4. **Answer**: 68.26% are acceptable

**Interview Insight**: Tests understanding of practical applications!

#### 2. Exponential Distribution - Waiting Times
**Intuition**: Time until next event when events happen at constant rate

**Mathematical Definition**:
```
X ~ Exp(Œª) where:
Œª = rate parameter (events per time unit)
X = waiting time until next event
```

**Probability Density Function**:
f(x) = Œª √ó e^(-Œªx), x ‚â• 0

**Cumulative Distribution Function**:
P(X ‚â§ x) = 1 - e^(-Œªx)

**Key Properties**:
- **Mean**: E[X] = 1/Œª
- **Variance**: Var(X) = 1/Œª¬≤
- **Memoryless Property**: P(X > s+t | X > s) = P(X > t)

**Memoryless Property Explained**:
"The probability of waiting additional time t doesn't depend on how long you've already waited!"

**Real-World Applications**:
- Time between customer arrivals
- Time until machine failure
- Time between website hits

#### Exponential Interview Questions

##### Q1: Customer Service Analysis
**Question**: Average customer arrival rate: 10 per hour. What's P(next customer arrives within 5 minutes)?

**Solution**:
Œª = 10 per hour = 10/60 per minute = 1/6 per minute
P(X ‚â§ 5) = 1 - e^(-Œªx) = 1 - e^(-(1/6)√ó5) = 1 - e^(-5/6) ‚âà 0.565

**Follow-up**:
- What's P(no customers for 15 minutes? P(X > 15) = e^(-15/6) ‚âà 0.082)
- How does this relate to Poisson? (Exponential models inter-arrival times, Poisson models count)

##### Q2: Memoryless Property Demonstration
**Question**: If you've waited 10 minutes for a bus that comes on average every 15 minutes, what's P(bus arrives in next 5 minutes)?

**Using Memoryless Property**:
P(X ‚â§ 15 | X > 10) = P(X ‚â§ 5) = 1 - e^(-5/15) = 1 - e^(-1/3)

**Interview Insight**: This demonstrates understanding of memoryless property!

### Distribution Selection Guide

| Situation | Distribution | Why? | Key Parameters |
|-----------|--------------|------|----------------|
| Single trial, success/failure | Bernoulli | Basic building block | p |
| Fixed trials, count successes | Binomial | Multiple Bernoulli | n, p |
| Rare events, fixed interval | Poisson | Events independent, rare | Œª |
| Natural phenomena, measurements | Normal | CLT, common in nature | Œº, œÉ¬≤ |
| Waiting times between events | Exponential | Constant rate, memoryless | Œª |

### Interview Strategy for Distribution Questions

#### Step-by-Step Approach:
1. **Identify the scenario**: What are we measuring?
2. **Choose appropriate distribution**: Based on problem characteristics
3. **Identify parameters**: n, p, Œª, Œº, œÉ
4. **Set up equation**: Use correct PMF/PDF
5. **Calculate systematically**: Show each step
6. **Interpret results**: What does this mean in context?

#### Common Interview Mistakes:
‚ùå Using binomial when Poisson is appropriate
‚ùå Forgetting to check distribution assumptions
‚ùå Confusing rate Œª with mean in exponential
‚ùå Not standardizing normal distributions
‚ùå Mixing up PMF and CDF

#### Pro Tips:
‚úÖ Always state assumptions clearly
‚úÖ Check if discrete or continuous first
‚úÖ Remember key properties (mean, variance)
‚úÖ Connect to real-world applications
‚úÖ Practice mental approximations

### Advanced Topics (Bonus for Strong Candidates)

#### 1. Distribution Relationships
```
Binomial ‚Üí Poisson (when n large, p small, np = Œª)
Binomial ‚Üí Normal (when n large, p not too close to 0 or 1)
Exponential ‚Üí Gamma (sum of exponential variables)
Normal ‚Üí Chi-square (sum of squared normal variables)
```

#### 2. Moment Generating Functions
**Why Important**: Uniquely characterizes distributions, useful for proving theorems

**Key Property**: If two distributions have same MGF, they are identical

#### 3. Truncated Distributions
**Real Application**: Censored data, bounded measurements

### Quick Reference Formula Sheet

#### Discrete Distributions:
```
Bernoulli: P(X=k) = p·µè(1-p)¬π‚Åª·µè, E[X]=p, Var[X]=p(1-p)
Binomial: P(X=k) = C(n,k)p·µè(1-p)‚Åø‚Åª·µè, E[X]=np, Var[X]=np(1-p)
Poisson: P(X=k) = (Œª·µèe‚ÅªŒª)/k!, E[X]=Œª, Var[X]=Œª
```

#### Continuous Distributions:
```
Normal: f(x) = (1/‚àö(2œÄœÉ¬≤))e^(-(x-Œº)¬≤/(2œÉ¬≤))
Standard Normal: Z = (X-Œº)/œÉ
Exponential: f(x) = Œªe^(-Œªx), E[X]=1/Œª, Var[X]=1/Œª¬≤
```

**Real Interview Question**: "A card is drawn from a standard deck. What's the probability it's a heart given that it's a face card?"

---

## Probability Distributions

### Key Distributions

**1. Discrete Distributions:**
- **Bernoulli**: Single trial with P(success) = p, P(failure) = 1-p
- **Binomial**: n independent Bernoulli trials
  - P(X = k) = C(n,k) √ó p^k √ó (1-p)^(n-k)
- **Poisson**: Number of events in fixed interval
  - P(X = k) = (Œª^k √ó e^(-Œª)) / k!

**2. Continuous Distributions:**
- **Normal**: Mean Œº, variance œÉ¬≤
  - f(x) = (1/‚àö(2œÄœÉ¬≤)) √ó e^(-(x-Œº)¬≤/(2œÉ¬≤))
- **Exponential**: Time between events
  - f(x) = Œª √ó e^(-Œªx), x ‚â• 0

### Properties to Remember

| Distribution | Mean | Variance | Key Use Case |
|--------------|------|----------|-------------|
| Bernoulli | p | p(1-p) | Single success/failure |
| Binomial | np | np(1-p) | Fixed trials, success counting |
| Poisson | Œª | Œª | Rare event counting |
| Normal | Œº | œÉ¬≤ | Natural phenomena |
| Exponential | 1/Œª | 1/Œª¬≤ | Waiting times |

### Interview Questions

#### Q1: Calls to a call center follow Poisson with Œª=2/minute. What's P(no calls in 3 minutes)?
**Solution**: For 3 minutes: Œª' = 2√ó3 = 6
P(X=0) = (6^0 √ó e^(-6)) / 0! = e^(-6) ‚âà 0.00248

**Follow-up**: What's P(at least 2 calls)? What's expected calls in 10 minutes?

#### Q2: Heights follow N(170, 25) in cm. What % > 175cm?
**Solution**: Z = (175-170)/‚àö25 = 5/5 = 1
P(Z > 1) = 1 - 0.8413 = 0.1587 or 15.87%

**Real Interview Question**: "A manufacturing process has 2% defect rate. What's the probability of exactly 3 defects in a batch of 100 items?"

---

## Sampling

### Key Concepts

**Sampling Methods:**
```
Simple Random Sample (SRS)
‚îú‚îÄ‚îÄ Each unit has equal probability
‚îî‚îÄ‚îÄ Selection is independent

Stratified Sampling
‚îú‚îÄ‚îÄ Population divided into strata
‚îî‚îÄ‚îÄ Random sample from each stratum

Systematic Sampling
‚îú‚îÄ‚îÄ Select every kth unit
‚îî‚îÄ‚îÄ Random starting point
```

**Sample Statistics:**
- **Sample Mean**: xÃÑ = Œ£xi / n
- **Sample Variance**: s¬≤ = Œ£(xi - xÃÑ)¬≤ / (n-1)
- **Standard Error**: SE = œÉ/‚àön

### Interview Questions

#### Q1: Population mean = 50, œÉ = 10. Sample size = 25. What's standard error?
**Solution**: SE = œÉ/‚àön = 10/5 = 2

**Follow-up**: What's probability sample mean > 52? How does SE change with n=100?

#### Q2: Explain stratified sampling vs simple random sampling.
**Answer**: Stratified ensures representation of all subgroups, reduces variance, better for heterogeneous populations.

**Real Interview Question**: "You have a dataset of 1M users. How would you sample 10K for analysis while ensuring representation across age groups?"

---

## Correlations

### Key Concepts

**Correlation Coefficient (r):**
- Range: [-1, 1]
- r = 1: Perfect positive linear
- r = -1: Perfect negative linear
- r = 0: No linear correlation

**Formula**: r = Œ£((xi - xÃÑ)(yi - »≥)) / ‚àö(Œ£(xi - xÃÑ)¬≤ √ó Œ£(yi - »≥)¬≤)

**Correlation Types:**
```
Pearson: Linear relationships
Spearman: Monotonic relationships (rank-based)
Kendall: Ordinal associations
```

### Interview Questions

#### Q1: Data: x=[1,2,3,4,5], y=[2,4,5,4,5]. Calculate correlation.
**Solution**: xÃÑ=3, »≥=4, calculate using formula ‚Üí r ‚âà 0.7

**Follow-up**: What if we square all x-values? How does correlation change?

#### Q2: Can correlation = 0 for nonlinearly related variables?
**Answer**: Yes! Example: y = x¬≤ for x ‚àà [-2,2]. Strong nonlinear relationship but r = 0.

**Real Interview Question**: "You find correlation 0.8 between study hours and test scores. Can we conclude that more studying causes better scores?"

---

## Hypothesis Testing

### Key Concepts

**Hypothesis Structure:**
- **H‚ÇÄ (Null)**: No effect/difference
- **H‚ÇÅ (Alternative)**: Effect/difference exists

**Test Statistics:**
- **Z-test**: Population œÉ known, n ‚â• 30
- **t-test**: Population œÉ unknown, n < 30
- **Chi-square**: Categorical data

**Decision Rules:**
- **Reject H‚ÇÄ**: |test statistic| > critical value
- **p-value approach**: Reject H‚ÇÄ if p < Œ±

### Common Tests

| Test | Purpose | Assumptions |
|------|---------|-------------|
| One-sample t | Œº vs hypothesized value | Normality, œÉ unknown |
| Two-sample t | Œº‚ÇÅ vs Œº‚ÇÇ | Normality, equal variances |
| Paired t | Before/after | Paired data, normality |
| Chi-square | Goodness of fit | Expected frequencies ‚â• 5 |

### Interview Questions

#### Q1: H‚ÇÄ: Œº = 100, Œ± = 0.05, two-tailed. Sample: n=25, xÃÑ=105, s=10. Test hypothesis.
**Solution**: t = (105-100)/(10/‚àö25) = 5/2 = 2.5
Critical t(24, 0.025) = ¬±2.064. |2.5| > 2.064 ‚Üí Reject H‚ÇÄ

**Follow-up**: Calculate p-value. What if n=100? What's Type I/II error risk?

#### Q2: When to use t-test vs z-test?
**Answer**: t-test when œÉ unknown or n < 30, z-test when œÉ known and n ‚â• 30.

**Real Interview Question**: "A/B test shows conversion rates: Control=10%, Treatment=12%, n=1000 each. Is the difference significant at Œ±=0.05?"

---

## Statistical Significance

### Key Concepts

**p-value**: Probability of observing extreme results if H‚ÇÄ is true

**Significance Level (Œ±)**: Pre-determined threshold (usually 0.05)

**Decision Matrix:**
```
                Reality
                H‚ÇÄ True    H‚ÇÄ False
     Reject     Type I    Correct
     Not Reject Correct   Type II
```

**Effect Size**: Magnitude of difference, independent of sample size

### Common Œ± Levels and Interpretation

| p-value range | Interpretation |
|---------------|----------------|
| p > 0.05 | Not significant |
| 0.01 < p ‚â§ 0.05 | Significant |
| 0.001 < p ‚â§ 0.01 | Highly significant |
| p ‚â§ 0.001 | Very highly significant |

### Interview Questions

#### Q1: p = 0.03 vs Œ± = 0.01. What's the decision?
**Answer**: Do not reject H‚ÇÄ (0.03 > 0.01)

**Follow-up**: What if we change Œ± to 0.05? What's the risk of Type I error?

#### Q2: "Statistically significant but practically insignificant" - explain.
**Answer**: Large sample size can detect tiny differences that are statistically significant but have no practical importance.

**Real Interview Question**: "Your A/B test shows p=0.04 favoring version B. Should we automatically roll out version B?"

---

## Central Limit Theorem

### Key Concept

**CLT**: For any distribution with mean Œº and variance œÉ¬≤, the sampling distribution of xÃÑ approaches Normal(N(Œº, œÉ¬≤/n)) as n ‚Üí ‚àû

**Practical Rule**: For most distributions, CLT works well for n ‚â• 30

### Mathematical Foundation

```
Sample Mean Distribution:
Œº_xÃÑ = Œº (unbiased estimator)
œÉ_xÃÑ¬≤ = œÉ¬≤/n (variance decreases with n)

Standard Error: SE = œÉ/‚àön
```

### Interview Questions

#### Q1: Population: Exponential with mean=2. Sample size=50. What's distribution of sample mean?
**Answer**: Approximately Normal with Œº=2, œÉ¬≤/50 = 4/50 = 0.08

**Follow-up**: What's P(xÃÑ > 2.5)? How does this change with n=100?

#### Q2: Why is CLT important in statistics?
**Answer**: Enables inference about population parameters from sample statistics, forms basis for hypothesis testing and confidence intervals.

**Real Interview Question**: "You have a skewed income distribution. Why can we still use t-tests for means with large samples?"

---

## Paired Means Tests

### Key Concept

**Paired t-test**: Compares two related samples (before/after, matched pairs)

**Test Statistic**: t = (dÃÑ - Œº‚ÇÄ) / (sd/‚àön)

Where dÃÑ = mean of differences, sd = standard deviation of differences

### When to Use Paired Tests

```
‚úì Same subjects measured twice
‚úì Matched pairs (twins, matched groups)
‚úì Before-after experiments
‚úì Case-control studies
```

### Interview Questions

#### Q1: Weight before: [180,170,165], after: [175,168,160]. Test if weight loss significant.
**Solution**: Differences: [-5,-2,-5], dÃÑ = -4, sd = 1.73, n = 3
t = (-4-0)/(1.73/‚àö3) = -4.01. Critical t(2,0.05) = -2.92
|t| > |critical| ‚Üí Significant weight loss

**Follow-up**: Calculate 95% CI for mean difference. What assumptions are made?

#### Q2: Paired vs independent t-test - when to use which?
**Answer**: Paired for related measurements, independent for separate groups. Paired reduces variability by controlling for subject effects.

**Real Interview Question**: "You want to test if a new training program improves employee productivity. How would you design the study and which test would you use?"

---

## Bayes' Theorem

### Formula and Intuition

**Bayes' Theorem**: P(A|B) = P(B|A) √ó P(A) / P(B)

**Components**:
- **Prior**: P(A) - initial belief
- **Likelihood**: P(B|A) - probability of evidence given hypothesis
- **Posterior**: P(A|B) - updated belief after evidence

### Practical Form

```
P(A|B) = [P(B|A) √ó P(A)] / [P(B|A) √ó P(A) + P(B|A') √ó P(A')]
```

### Interview Questions

#### Q1: Disease prevalence = 1%, Test accuracy = 99%. Test positive. What's probability of having disease?
**Solution**:
P(Disease|+) = (0.99 √ó 0.01) / [(0.99 √ó 0.01) + (0.01 √ó 0.99)]
= 0.0099 / (0.0099 + 0.0099) = 0.5 = 50%

**Follow-up**: What if prevalence = 0.1%? What if test accuracy = 95%?

#### Q2: Two coins: one fair, one double-headed. Pick random coin, flip heads. What's probability it's double-headed?
**Solution**: P(Double|H) = (1 √ó 0.5) / [(1 √ó 0.5) + (0.5 √ó 0.5)] = 2/3

**Real Interview Question**: "Your ML model has 95% accuracy. In a population where 2% are fraud cases, a positive prediction occurs. What's the actual probability of fraud?"

---

## Linear Algebra

### Key Concepts

**Matrix Operations**:
- **Addition**: Element-wise
- **Multiplication**: (A√óB)·µ¢‚±º = Œ£(A·µ¢‚Çñ √ó B‚Çñ‚±º)
- **Inverse**: A‚Åª¬π such that A√óA‚Åª¬π = I
- **Transpose**: (A·µÄ)·µ¢‚±º = A‚±º·µ¢

**Eigenvalues and Eigenvectors**:
```
A √ó v = Œª √ó v
Where: v = eigenvector, Œª = eigenvalue
```

**Singular Value Decomposition (SVD)**:
```
A = U √ó Œ£ √ó V·µÄ
U: Left singular vectors (orthogonal)
Œ£: Singular values (diagonal)
V·µÄ: Right singular vectors (orthogonal)
```

### Applications in ML

| Concept | ML Application | Why Important |
|---------|----------------|----------------|
| Eigenvalues | PCA | Find principal components |
| SVD | Dimensionality reduction | Compress data efficiently |
| Matrix inverse | Linear regression | Solve normal equations |
| Matrix multiplication | Neural networks | Weight transformations |

### Interview Questions

#### Q1: Matrix A = [[2,1],[1,2]]. Find eigenvalues and eigenvectors.
**Solution**:
Characteristic equation: |A - ŒªI| = 0
(2-Œª)(2-Œª) - 1 = 0 ‚Üí Œª¬≤ - 4Œª + 3 = 0
Eigenvalues: Œª‚ÇÅ = 3, Œª‚ÇÇ = 1

For Œª=3: (A-3I)v = 0 ‚Üí [[-1,1],[1,-1]]v = 0 ‚Üí v‚ÇÅ = [1,1]
For Œª=1: (A-I)v = 0 ‚Üí [[1,1],[1,1]]v = 0 ‚Üí v‚ÇÇ = [1,-1]

**Follow-up**: What's the spectral decomposition? How does this relate to PCA?

#### Q2: Why is SVD important in machine learning?
**Answer**:
- Dimensionality reduction (keep top k singular values)
- Matrix completion/recommendation systems
- Numerical stability vs eigenvalue decomposition
- Works for any matrix (not just square)

**Real Interview Question**: "How would you use matrix decomposition to reduce the dimensionality of a user-item interaction matrix for recommendation?"

---

## Quick Reference Formulas

### Probability
- **Conditional**: P(A|B) = P(A‚à©B)/P(B)
- **Independence**: P(A‚à©B) = P(A)√óP(B)
- **Total Probability**: P(A) = Œ£P(A|B·µ¢)P(B·µ¢)

### Distributions
- **Binomial**: P(X=k) = C(n,k) √ó p·µè √ó (1-p)‚Åø‚Åª·µè
- **Poisson**: P(X=k) = Œª·µè √ó e‚ÅªŒª / k!
- **Normal**: f(x) = (1/‚àö(2œÄœÉ¬≤)) √ó e^(-(x-Œº)¬≤/(2œÉ¬≤))

### Statistics
- **Sample Mean**: xÃÑ = Œ£xi/n
- **Sample Variance**: s¬≤ = Œ£(xi-xÃÑ)¬≤/(n-1)
- **Correlation**: r = Œ£((xi-xÃÑ)(yi-»≥))/‚àö(Œ£(xi-xÃÑ)¬≤ √ó Œ£(yi-»≥)¬≤)
- **t-statistic**: t = (xÃÑ-Œº‚ÇÄ)/(s/‚àön)

### Matrix Operations
- **Matrix Multiplication**: (AB)·µ¢‚±º = Œ£‚Çñ A·µ¢‚ÇñB‚Çñ‚±º
- **SVD**: A = UŒ£V·µÄ
- **Eigenvalue**: det(A-ŒªI) = 0

---

## Practice Problems for Self-Assessment

### Set 1: Mixed Topics
1. Coin tossed until first head. Expected tosses? (Answer: 2)
2. Distribution of sample mean for n=36 from skewed population? (Answer: Approximately normal)
3. Correlation vs causation example from data science?

### Set 2: Advanced Applications
1. Naive Bayes classifier derivation from Bayes' theorem
2. PCA derivation using eigenvalue decomposition
3. Bootstrap method for confidence intervals

### Set 3: Interview-Style Questions
1. "How would you detect outliers in a dataset?"
2. "Explain the bias-variance tradeoff mathematically."
3. "When would you use median instead of mean?"

---

## Final Tips for the Interview

### Common Pitfalls to Avoid:
- ‚ùå Confusing correlation with causation
- ‚ùå Ignoring assumptions of statistical tests
- ‚ùå Forgetting to mention practical significance
- ‚ùå Not explaining the intuition behind formulas

### What Interviewers Look For:
- ‚úÖ Clear mathematical reasoning
- ‚úÖ Understanding of assumptions and limitations
- ‚úÖ Practical application knowledge
- ‚úÖ Ability to explain complex concepts simply

### Time Management:
- üìä **33.33% weightage** for this section
- ‚è±Ô∏è **30 minutes** allocated
- üéØ Aim for 2-3 detailed solutions + multiple quick concepts
- üìù Show your work clearly and systematically